{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM的英文叫做Support Vector Machine，中文名字叫做支持向量机。它是常见的一种分类方法，在机器学习中，SVM是有监督的学习模型。\n",
    "\n",
    "什么是有监督的学习模型？它指的是我们需要事先对数据打上分类的标签，这样机器就知道这个数据是属于哪个分类。同样无监督学习，就是数据没有被打上分类标签，这可能是因为我们不具备先验的知识，或者打标签的成本很高。所以我们需要机器代替我们部分完成这个工作，比如将数据进行聚类，方便后续的人工对每个类进行分析。SVM作为有监督的学习模型，通常可以帮助我们模式识别、分类以及回归分析。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子：最典型的例子就是将红蓝小球混合在一起放在桌面，使用超平面将小球划分成两部分。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM 的工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用SVM计算的过程就是帮我们找到那个超平面的过程，这个超平面就是我们的SVM分类器。\n",
    "\n",
    "再回过头来看最简单的例子如下，其实我们可以有多种直线的划分，比如下图的直线A、直线B、直线C,究竟哪种才是更好的划分？\n",
    "\n",
    "![](svm划分.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很明显图中的直线B更加接近蓝色球，但是在真实的环境下，球再多一些的话，蓝色球可能就会被划分到了直线B的右侧，被认为是红色的球。同样直线A更接近红球，在真实的环境下，如果红球再多一点，也可能会被认为是蓝色的球。所以相比于直线A和直线B，直线C的划分更优，因为它的鲁棒性更强。\n",
    "\n",
    "那么怎样才能寻找到直线C这个更优的答案呢？这里，引入一个SVM特有的概念：分词间隔。\n",
    "\n",
    "实际上，我们的分类环境不是在二维平面中的，而是在多为空间中的，这样直线C就变成了决策面C。\n",
    "\n",
    "在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面C，直到产生两个极限的位置：如图中的决策面A和决策面B。极限的位置是指，如果超过这个位置，就会产生分类的错误。这样的话，两个极限位置A和B之间的分界线C就是最优决策面。极限位置到最优决策面C之间的距离，就是“分类间隔”，英文叫做margin。\n",
    "\n",
    "如果我们转动这个最优决策面，你就会发现可能存在多个最优决策面，他们都能够把数据集正确分开，这些最优的决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是SVM要找的最优解。\n",
    "![](最优平面.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 点到超平面的距离公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，如果我们把红蓝两种颜色的球放到一个三维的空间里，我们发现决策面就变成一个平面。这里我们可以使用线性函数来表示，如果在一维空间里面就表示一个点，在二维空间里面就表示一条直线，在单位空间里面就表示一个平面，当然空间的位数还可以更多，这里我们给这个线性函数起个名称叫做超平面。数学公式可以表示成：\n",
    "$$g(x)=w^Tx + b,其中w，x属于R^n$$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个公式里面，w，x是n维空间里的向量，其中x是函数变量；w是法向量。法向量这里指的是垂直于平面的直线所表示的向量，它决定了超平面的方向。\n",
    "\n",
    "**SVM就是帮助我们找到一个超平面**，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。\n",
    "\n",
    "在这个过程中，支持向量就是离分类超平面最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实分类都没有意义。\n",
    "\n",
    "所以说，SVM就是求解最大分类间隔的过程，我们还需要对分类间隔的大小进行定义。\n",
    "\n",
    "首先，我们定义某类样本集到超平面的距离是这个样本集合内的样本到超平面的最短距离。我们用di代表点xi到超平面wxi + b = 0的欧氏距离。因此我们要求di的最小值，用它来代表这个样本到超平面的最短距离。di可以使用计算公式得到：\n",
    "$$d_i = \\frac{|wx_i+b|}{||w||} $$\n",
    "\n",
    "其中||w||为超平面的范数，di的公式可以用解析集合的知识进行推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最大间隔的优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的目标就是找出所有的分类间隔中最大的那个值对应的超平面。在数学上，这是一个凸优化问题（凸优化就是关于求凸集中的凸函数最小化的问题）。通过凸优化问题，最后可以求得最优化的w和b，也就是我们想要找到的最优超平面。中间求解的过程会用到拉格朗日乘子。和KKT（Karush-Kuhu-Tucker）条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 硬间隔、软间隔和非线性SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定数据是完全的线性可分的，那么学习到的模型可以称之为硬间隔支持向量机。换个说法，**硬间隔值的就暗示完全分类准确，不存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。**\n",
    "\n",
    "实际工作中的数据没有那么“干净”，或多或少都会存在一些噪点。所以线性可分是个例行情况。这时我们需要使用到软间隔SVM（近似线性可分），例如：\n",
    "![](近似线性可分.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外还存在一种情况，就是非线性支持向量机。\n",
    "\n",
    "比如下面的样本集就是个非线性的数据。图中的两类数据，分别分布为两个圆圈的形状，那么这种情况下，不论是多么高级的分类器，只要映射函数是线性的，就没法处理，SVM也处理不了。\n",
    "\n",
    "这是后需要引入一个新的概念：**核函数。它可以将样本从原始的空间映射到一个更加高维度的特质空间中，使得样本在新的空间中线性可分。**这样我们就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。\n",
    "![](非线性数据.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以在非线性的SVM中，核函数的选择就是影响SVM最大的变量。最常用的核函数有线性核、多项式核、高斯核、拉普拉斯核、sigmoid核，或者是这些核函数的组合，这些函数的区别在于映射的方式不同，通过这些核函数，我们就可以把样本空间投射到新的高维空间中。\n",
    "\n",
    "当然软间隔和核函数的提出，都是为了方便我么对上面的超平面的公式中的w*和b*进行求解，从而得到最大分类间隔的超平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用SVM如何解决多分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM本身是一个二值分类器，最初始为了二分类问题设计的，也就是回答Yes或者No。而实际上我们要解决的问题，可能是多分类问题，比如对文本进行分类，或者对图像进行识别。\n",
    "\n",
    "针对这种情况，我们可以将多个二分类组合器组合起来形成一个多分类器，常见的方法有“一对多”和“一对一”两种。\n",
    "\n",
    "**1、一对多**\n",
    "假设我们把物体分成A、B、C、D四种类型，那么我们就可以先把其中的一类作为分类1，其他类统一归为分类2.这样我们可以构造出4种SVM，分别为以下的情况。\n",
    "\n",
    "(1)样本A作为正集，B，C，D作为负集；\n",
    "\n",
    "(1)样本B作为正集，A，C，D作为负集；\n",
    "\n",
    "(1)样本C作为正集，A，B，D作为负集；\n",
    "\n",
    "(1)样本D作为正集，A，B，C作为负集；\n",
    "\n",
    "这种方法，针对K个分类，需要训练K个分类器，分类速度较快，但是训练的速度较慢，因为每个分类器都需要对全体的样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对成的情况，而且当增加新的分类，比如第K+1类的时候，需要重新对分类器进行构造。\n",
    "\n",
    "**2、一对一法**\n",
    "一对一法的初衷是想在训练的时候更加灵活。我们可以在任意两类样本之间构造一个SVM，这样针对K类的样本，就会有C（k，2）类分类器。\n",
    "\n",
    "比如我们想要划分A、B、C三个类，可以构造3个分类器：\n",
    "\n",
    "(1)分类器1：A、B；\n",
    "\n",
    "(2)分类器2：A、C；\n",
    "\n",
    "(3)分类器3：B、C；\n",
    "\n",
    "当对一个未知的样本进行分类的时候，每一个分类器都会有一个分类结果，即为1票，最终的得票最多的类别就是整个未知样本的类别。\n",
    "\n",
    "这样做的好处是，如果新增一类，不需要重新训练所有的SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个SVM模型的时候，训练的熟读极快。\n",
    "\n",
    "但是这种方法的不足在于，分类器的个数与K的平方成正比，所以当K较大时候，训练和测试的时间会比较慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要掌握三个维度：\n",
    "- 1、完全线性可分的情况下的线性分类器，也就是线性可分的情况，是最原始的SVM，它最核心的思想就是找到最大的分类间隔；\n",
    "- 2、大部分线性可分的情况下的线性分类器，引入了软间隔的概念。软间隔，就是允许一定量的样本分类错误；\n",
    "- 3、线性不可分的情况下的非线性分类器，引入了核函数。它让原有的样本空间通过核函数投射到了一个高维度的空间中，从而变得线性可分。\n",
    "\n",
    "监督学习和非监督学习\n",
    "- 前者是已知类别的情况，贴上红蓝标签，按照标签进行分类\n",
    "- 后者是不是道类别，不贴上红蓝标签，自己学会识别红色和蓝色，然后分类\n",
    "\n",
    "硬间隔：完美数据下的完美情况，分出完美分类\n",
    "软间隔：总监总是混杂在一起，总有杂质，情况总是复杂，分类总是有一点错误\n",
    "核函数：是从低纬度到高维度的的映射关系。如果从高到低压缩的话就会使得结果可能变得混乱不可分，但是从低维到高维，属性的维度增加了，但是可以在另外一个空间中变得线性可分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
